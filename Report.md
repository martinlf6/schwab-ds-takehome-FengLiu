Sentiment Analysis Results
Sentence Sentiment Analysis
Reports
Classification Report (Top Table)
This shows precision, recall, F1-score, and support for each class.
â€¢	Negative (61 samples)
o	Precision = 0.98 â†’ When the model predicts negative, 98% are correct.
o	Recall = 0.92 â†’ Of all true negative cases, 92% are captured (8% missed, misclassified as positive).
o	F1-score = 0.95 â†’ Balance between precision & recall.
â€¢	Neutral (278 samples)
o	Precision = 1.00 â†’ Every time the model predicts neutral, itâ€™s always correct.
o	Recall = 0.99 â†’ Almost all true neutral cases are identified (only 1% missed).
o	F1-score = 0.99 â†’ Very strong performance here.
â€¢	Positive (114 samples)
o	Precision = 0.93 â†’ When predicting positive, 93% are correct (7% false positives).
o	Recall = 0.98 â†’ Almost all true positives are captured.
o	F1-score = 0.95 â†’ Solid balance.
Overall metrics
â€¢	Accuracy = 0.98 â†’ Out of all 453 test samples, 98% are correct.
â€¢	Macro avg = averages metrics equally across classes â†’ F1 = 0.96
â€¢	Weighted avg = averages weighted by class size â†’ F1 = 0.98 (higher because neutral dominates with 278 samples and is predicted very well).
Confusion Matrix (Bottom Plot)
This shows actual vs predicted counts:
â€¢	Row = True class, Column = Predicted class
o	Negative (61 true):
ï‚§	56 correctly predicted as negative
ï‚§	5 misclassified as positive
o	Neutral (278 true):
ï‚§	274 correctly predicted
ï‚§	4 misclassified as positive
o	Positive (114 true):
ï‚§	112 correctly predicted
ï‚§	1 misclassified as negative
ï‚§	1 misclassified as neutral
ðŸ‘‰ Errors are very few and mostly between negative â†” positive or neutral â†” positive (natural since theyâ€™re semantically closer).
Key Takeaways
1.	The model performs exceptionally well (98% accuracy, F1 > 0.95 across all classes).
2.	Neutral is almost perfectly predicted (possibly because itâ€™s the majority class).
3.	Negative has slightly lower recall (0.92) â†’ some negatives get mistaken as positives.
4.	Overall balance is strong, meaning class imbalance is not severely hurting performance.
Business/Real-World Interpretation
Why These Metrics Matter in Business Context
1. Negative Class (Customer complaints, risk alerts, bad financial news)
â€¢	Recall = 0.92 â†’ The model catches 92% of negatives, but misses 8%.
â€¢	Business impact: Missing a negative (false negative) could mean overlooking a serious issue (e.g., financial risk, compliance violation, or customer dissatisfaction).
â€¢	In risk-sensitive industries (finance, insurance, healthcare), high recall is crucial â†’ youâ€™d rather flag too many negatives than miss one.
2. Neutral Class (Normal or uninformative content)
â€¢	Precision = 1.00, Recall = 0.99 â†’ The model is almost perfect here.
â€¢	Business impact: Neutral predictions are low risk. If the model mistakes a few, it doesnâ€™t matter much since these donâ€™t trigger critical business actions.
â€¢	High accuracy here is good, but less critical compared to negative or positive.
3. Positive Class (Opportunities, good sentiment, positive financial signals)
â€¢	Precision = 0.93, Recall = 0.98 â†’ Very strong performance.
â€¢	Business impact: If this model is used for market sentiment or customer engagement, then missing positives (false negatives) could mean overlooking opportunities (e.g., happy customers you could upsell, bullish market news).
â€¢	Good recall ensures you donâ€™t miss many positives, but slightly lower precision means a few false positives (things flagged as positive but not truly so).
Metric Priorities Depend on Use Case
â€¢	If used for risk detection (compliance, fraud, complaints):
o	Maximize recall for negatives â†’ better to catch every possible negative, even if some are false alarms.
o	A false positive is annoying, but a false negative could cost millions.
â€¢	If used for opportunity mining (marketing, investments, client engagement):
o	Maximize recall for positives â†’ donâ€™t miss good opportunities.
o	Slightly lower precision is acceptable, because reviewing extra "positives" is less costly than missing a big chance.
â€¢	If used for overall sentiment monitoring (balanced insights):
o	The current model is already well-balanced â†’ 98% accuracy, F1 ~0.95+ across classes means itâ€™s strong enough for dashboards, analytics, and trend reporting.
Summary in Business Terms:
â€¢	The model is excellent overall, but depending on your business goal, you might want to tune the loss function or decision thresholds to prioritize recall for negatives (risk detection) or positives (opportunity spotting).
â€¢	For general analytics, this model is already production-ready.

